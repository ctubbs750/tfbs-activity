Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job                       count
----------------------  -------
all                           1
unibind_flatten_dir           1
unibind_unpack_unibind        1
total                         3

Select jobs to execute...

[Sun Dec 24 09:12:53 2023]
Job 2: 
        Unpacks UniBind data download
        
Reason: Set of input files has changed since last execution; Code has changed since last execution

Terminating processes on user request, this might take some time.
[Sun Dec 24 09:15:12 2023]
Error in rule unibind_unpack_unibind:
    jobid: 2
    input: resources/data/unibind/damo_hg39_all_TFBS.tar.gz
    output: resources/data/unibind/damo_hg38_all_TFBS_unpacked
    log: workflow/logs/unpack_unibind.stdout, workflow/logs/unpack_unibind.stderr (check log file(s) for error details)
    shell:
        
        mkdir -p resources/data/unibind/damo_hg38_all_TFBS_unpacked && tar -xzf resources/data/unibind/damo_hg39_all_TFBS.tar.gz -C resources/data/unibind/damo_hg38_all_TFBS_unpacked
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: .snakemake/log/2023-12-24T091250.457167.snakemake.log
